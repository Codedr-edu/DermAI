# ğŸ”¢ QUANTIZATION LÃ€ GÃŒ VÃ€ CÃCH HOáº T Äá»˜NG

## ğŸ¯ GIáº¢I THÃCH Äá»† NHáº¤T

### Quantization = Giáº£m Ä‘á»™ chÃ­nh xÃ¡c sá»‘ Ä‘á»ƒ tiáº¿t kiá»‡m RAM

**VÃ­ dá»¥ Ä‘Æ¡n giáº£n:**

Báº¡n cÃ³ 1 con sá»‘: **3.14159265358979323846...**

**FP32 (Float32 - 32 bit):**
- LÆ°u sá»‘ vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao: `3.14159265`
- Má»—i sá»‘ chiáº¿m: **4 bytes** (32 bit)

**FP16 (Float16 - 16 bit):**
- LÆ°u sá»‘ vá»›i Ä‘á»™ chÃ­nh xÃ¡c tháº¥p hÆ¡n: `3.141`
- Má»—i sá»‘ chiáº¿m: **2 bytes** (16 bit)

**â†’ Tiáº¿t kiá»‡m: 50% memory!**

---

## ğŸ’¾ ÃP Dá»¤NG VÃ€O MODEL

### Model lÃ  gÃ¬?
Model = **HÃ€NG TRIá»†U** con sá»‘ (weights/parameters)

**VÃ­ dá»¥:**
```
Model cÃ³ 10 triá»‡u parameters
```

**FP32:**
```
10,000,000 params Ã— 4 bytes = 40 MB
```

**FP16 (quantized):**
```
10,000,000 params Ã— 2 bytes = 20 MB
â†’ Tiáº¿t kiá»‡m 50%!
```

---

## ğŸ§® MODEL Cá»¦A Báº N

### Hiá»‡n táº¡i:

```bash
$ ls -lh Dermal/dermatology_stage1.keras
95M
```

**â†’ Model Ä‘Ã£ Cá»°C Ká»² NHá» rá»“i!**

CÃ³ thá»ƒ:
- ÄÃ£ Ä‘Æ°á»£c quantize tá»« trÆ°á»›c
- Hoáº·c architecture nhá» gá»n
- Hoáº·c Ä‘Ã£ Ä‘Æ°á»£c optimize

**â†’ KHÃ”NG Cáº¦N quantize thÃªm!** âœ…

---

## ğŸ“Š SO SÃNH

| Type | Size per param | Example Model | Total Size |
|------|----------------|---------------|------------|
| **FP32** | 4 bytes | 100M params | ~400 MB |
| **FP16** | 2 bytes | 100M params | ~200 MB |
| **INT8** | 1 byte | 100M params | ~100 MB |

**Trade-off:**
- CÃ ng giáº£m Ä‘á»™ chÃ­nh xÃ¡c â†’ CÃ ng nhá»
- NhÆ°ng cÃ³ thá»ƒ giáº£m accuracy!

---

## ğŸ”§ CÃCH QUANTIZE (Náº¾U Cáº¦N)

### Method 1: TensorFlow Lite Converter (Post-training quantization)

```python
import tensorflow as tf

# Load model gá»‘c
model = tf.keras.models.load_model('model.keras')

# Convert sang TFLite vá»›i quantization
converter = tf.lite.TFLiteConverter.from_keras_model(model)

# Dynamic range quantization (FP32 â†’ FP16)
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# Convert
tflite_model = converter.convert()

# Save
with open('model_quantized.tflite', 'wb') as f:
    f.write(tflite_model)

print(f"Original size: {os.path.getsize('model.keras') / 1024 / 1024:.1f} MB")
print(f"Quantized size: {len(tflite_model) / 1024 / 1024:.1f} MB")
```

**Káº¿t quáº£:**
```
Original size: 400.0 MB
Quantized size: 200.0 MB  # Giáº£m 50%!
```

---

### Method 2: Mixed Precision (Trong TensorFlow)

```python
# Khi train model
from tensorflow.keras import mixed_precision

# Enable mixed precision (FP16)
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

# Train model nhÆ° bÃ¬nh thÆ°á»ng
model = create_model()
model.compile(...)
model.fit(...)
```

**Lá»£i Ã­ch:**
- Model tá»± Ä‘á»™ng dÃ¹ng FP16 cho tÃ­nh toÃ¡n
- Váº«n giá»¯ FP32 cho cÃ¡c operations quan trá»ng
- Tá»± Ä‘á»™ng convert

---

### Method 3: Manual Quantization (Advanced)

```python
import tensorflow as tf
import numpy as np

# Load model
model = tf.keras.models.load_model('model.keras')

# Get weights
weights = model.get_weights()

# Quantize má»—i weight tá»« FP32 â†’ FP16
quantized_weights = []
for w in weights:
    # Convert to FP16
    w_fp16 = w.astype(np.float16)
    quantized_weights.append(w_fp16)

# Set weights má»›i
model.set_weights(quantized_weights)

# Save
model.save('model_quantized.keras')
```

âš ï¸ **LÆ°u Ã½:** Method nÃ y Ä‘Æ¡n giáº£n nhÆ°ng cÃ³ thá»ƒ lÃ m giáº£m accuracy!

---

## ğŸ”¬ CHECK MODEL ÄÃƒ QUANTIZED CHÆ¯A

```python
import tensorflow as tf

# Load model
model = tf.keras.models.load_model('Dermal/dermatology_stage1.keras')

# Check dtype cá»§a weights
for layer in model.layers:
    if hasattr(layer, 'weights') and layer.weights:
        for weight in layer.weights:
            print(f"{layer.name}: {weight.dtype}")
            break  # Chá»‰ check weight Ä‘áº§u
        break  # Chá»‰ check layer Ä‘áº§u
```

**Output:**
```python
# Náº¿u FP32 (chÆ°a quantize):
conv2d: <dtype: 'float32'>

# Náº¿u FP16 (Ä‘Ã£ quantize):
conv2d: <dtype: 'float16'>
```

---

## ğŸ’¡ KHI NÃ€O Cáº¦N QUANTIZE?

### âœ… NÃŠN quantize náº¿u:

1. **Model > 500MB**
   ```
   Model: 1000 MB
   RAM limit: 512 MB
   â†’ PHáº¢I quantize!
   ```

2. **Memory usage quÃ¡ cao**
   ```
   Peak memory: 600 MB > 512 MB
   â†’ Quantize Ä‘á»ƒ giáº£m xuá»‘ng
   ```

3. **Deploy lÃªn mobile/edge devices**
   ```
   Mobile RAM: 2-4 GB
   Model FP32: 500 MB
   â†’ Quantize Ä‘á»ƒ app mÆ°á»£t hÆ¡n
   ```

### âŒ KHÃ”NG Cáº¦N quantize náº¿u:

1. **Model Ä‘Ã£ nhá» (<200MB)**
   ```
   Model cá»§a báº¡n: 95 MB âœ…
   Peak memory: 400 MB < 512 MB âœ…
   â†’ KHÃ”NG Cáº¦N quantize!
   ```

2. **RAM Ä‘á»§ dÆ°**
   ```
   Available: 512 MB
   Usage: 400 MB
   Buffer: 112 MB âœ…
   ```

3. **Accuracy quan trá»ng hÆ¡n size**
   ```
   Medical diagnosis model
   â†’ Giá»¯ FP32 Ä‘á»ƒ accuracy tá»‘i Ä‘a
   ```

---

## ğŸ“Š QUANTIZATION METHODS COMPARISON

| Method | Size Reduction | Accuracy Loss | Complexity |
|--------|----------------|---------------|------------|
| **FP32 â†’ FP16** | 50% | ~0-2% | Dá»… |
| **FP32 â†’ INT8** | 75% | ~2-5% | Trung bÃ¬nh |
| **FP32 â†’ INT4** | 87.5% | ~5-10% | KhÃ³ |
| **Pruning + Quant** | 90%+ | ~3-8% | Ráº¥t khÃ³ |

---

## ğŸ”§ SCRIPT QUANTIZE Tá»° Äá»˜NG

Táº¡o file `Dermal/quantize_model.py`:

```python
#!/usr/bin/env python3
"""
Quantize TensorFlow model tá»« FP32 sang FP16
Chá»‰ cháº¡y náº¿u model > 500MB!
"""

import tensorflow as tf
import os

def quantize_model(input_path, output_path):
    """Quantize model using TFLite converter"""
    
    # Check input file
    if not os.path.exists(input_path):
        print(f"âŒ Model khÃ´ng tá»“n táº¡i: {input_path}")
        return
    
    original_size = os.path.getsize(input_path) / (1024 * 1024)
    print(f"ğŸ“¦ Original model: {original_size:.1f} MB")
    
    # Náº¿u model < 200MB, khÃ´ng cáº§n quantize
    if original_size < 200:
        print(f"âœ… Model Ä‘Ã£ Ä‘á»§ nhá» ({original_size:.1f} MB < 200 MB)")
        print("   KhÃ´ng cáº§n quantize!")
        return
    
    print("ğŸ”„ Äang quantize model...")
    
    # Load model
    model = tf.keras.models.load_model(input_path, compile=False)
    
    # Convert to TFLite with quantization
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    
    # Float16 quantization
    converter.target_spec.supported_types = [tf.float16]
    
    # Convert
    tflite_model = converter.convert()
    
    # Save
    with open(output_path, 'wb') as f:
        f.write(tflite_model)
    
    quantized_size = len(tflite_model) / (1024 * 1024)
    reduction = (1 - quantized_size / original_size) * 100
    
    print(f"âœ… Quantized model: {quantized_size:.1f} MB")
    print(f"ğŸ“‰ Giáº£m: {reduction:.1f}%")
    print(f"ğŸ’¾ Saved to: {output_path}")

if __name__ == "__main__":
    INPUT = "Dermal/dermatology_stage1.keras"
    OUTPUT = "Dermal/dermatology_stage1_quantized.tflite"
    
    quantize_model(INPUT, OUTPUT)
```

**Cháº¡y:**
```bash
python Dermal/quantize_model.py
```

**Output:**
```
ğŸ“¦ Original model: 95.0 MB
âœ… Model Ä‘Ã£ Ä‘á»§ nhá» (95.0 MB < 200 MB)
   KhÃ´ng cáº§n quantize!
```

---

## ğŸ¯ Vá»šI MODEL Cá»¦A Báº N

### Model hiá»‡n táº¡i: 95MB

```
Django + Python:       70 MB
TensorFlow:           120 MB
Model (95MB):          95 MB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Idle:                 285 MB

+ Inference:          +30 MB
+ Grad-CAM:           +85 MB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Peak:                ~400 MB < 512 MB âœ…
```

**Káº¾T LUáº¬N:** KHÃ”NG Cáº¦N QUANTIZE!

### Náº¿u quantize xuá»‘ng 50MB:

```
Model (50MB):          50 MB (thay vÃ¬ 95MB)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Idle:                 240 MB (tiáº¿t kiá»‡m 45MB)
Peak:                ~355 MB (tiáº¿t kiá»‡m 45MB)
```

**Lá»£i Ã­ch:** Tiáº¿t kiá»‡m 45MB  
**Trade-off:** CÃ³ thá»ƒ máº¥t 1-2% accuracy  
**ÄÃ¡ng khÃ´ng?** KHÃ”NG! VÃ¬ Ä‘Ã£ dÆ° 112MB buffer rá»“i!

---

## ğŸ“ TÃ“M Láº I

### Quantization lÃ  gÃ¬?
â†’ Giáº£m Ä‘á»™ chÃ­nh xÃ¡c sá»‘ (FP32 â†’ FP16) Ä‘á»ƒ tiáº¿t kiá»‡m RAM

### Lá»£i Ã­ch?
â†’ Giáº£m 50% model size, giáº£m RAM usage

### Trade-off?
â†’ CÃ³ thá»ƒ giáº£m 1-5% accuracy

### Khi nÃ o cáº§n?
â†’ Khi model > 500MB hoáº·c RAM khÃ´ng Ä‘á»§

### Model 95MB cá»§a báº¡n?
â†’ **KHÃ”NG Cáº¦N quantize!** ÄÃ£ Ä‘á»§ nhá», peak 400MB < 512MB âœ…

---

## ğŸš€ ACTION

**Vá»›i model 95MB:**
```bash
# KHÃ”NG Cáº¦N cháº¡y quantize script
# Deploy trá»±c tiáº¿p vá»›i config:

PRELOAD_MODEL=false
ENABLE_GRADCAM=true     # âœ… Báº¬T Ä‘Æ°á»£c!
```

**Náº¿u sau nÃ y model lá»›n hÆ¡n (>500MB):**
```bash
# LÃºc Ä‘Ã³ má»›i cháº¡y
python Dermal/quantize_model.py

# Sau Ä‘Ã³ deploy vá»›i quantized model
```

---

## ğŸ’¡ HIá»‚U THÃŠM

### Táº¡i sao model 95MB nhá» tháº¿?

CÃ³ thá»ƒ:
1. **Architecture nhá»:** EfficientNet-B0/B1 (khÃ´ng pháº£i B7)
2. **ÄÃ£ quantize:** Ai Ä‘Ã³ Ä‘Ã£ quantize trÆ°á»›c khi train
3. **Few layers:** Model khÃ´ng sÃ¢u láº¯m
4. **Transfer learning:** Chá»‰ fine-tune layers cuá»‘i

### Check architecture:

```python
import tensorflow as tf

model = tf.keras.models.load_model('Dermal/dermatology_stage1.keras')
model.summary()

# Count parameters
total_params = model.count_params()
print(f"Total parameters: {total_params:,}")
print(f"Size per param: {95 * 1024 * 1024 / total_params:.2f} bytes")
```

**Náº¿u káº¿t quáº£ ~2 bytes/param:**
â†’ ÄÃ£ lÃ  FP16 rá»“i! (ÄÃ£ quantize)

**Náº¿u káº¿t quáº£ ~4 bytes/param:**
â†’ Váº«n FP32 (chÆ°a quantize, nhÆ°ng architecture nhá»)

---

RÃµ chÆ°a báº¡n? ğŸ˜Š
