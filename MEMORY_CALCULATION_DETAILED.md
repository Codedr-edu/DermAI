# üíæ T√çNH TO√ÅN RAM CHI TI·∫æT

## üîç PH√ÇN T√çCH K·ª∏ MEMORY USAGE

### ‚ùå T√çNH TO√ÅN C≈® (Qu√° bi quan!)

Tr∆∞·ªõc ƒë√¢y t√¥i t√≠nh:
```
Django:           ~100 MB
TensorFlow:       ~200 MB
Model (loaded):   ~1000 MB
Grad-CAM:         ~200 MB
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total:            ~1500 MB >>> 512 MB ‚ùå
```

**V·∫•n ƒë·ªÅ:** T√¥i c·ªông T·∫§T C·∫¢ l·∫°i nh∆∞ th·ªÉ ch√∫ng t·ªìn t·∫°i c√πng l√∫c!

---

## ‚úÖ T√çNH TO√ÅN ƒê√öNG (Ph√¢n bi·ªát Idle vs Peak)

### 1. Memory Idle (Kh√¥ng c√≥ request)

```
Django base:           ~80 MB
Python interpreter:    ~30 MB
TensorFlow runtime:    ~150 MB
Model (FP32, 1GB):    ~1000 MB
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
IDLE Total:           ~1260 MB >>> 512 MB ‚ùå
```

**‚Üí V·∫•n ƒë·ªÅ:** Model FP32 (1GB) qu√° l·ªõn!

---

### 2. Memory Idle (V·ªõi Quantized Model)

```
Django base:           ~80 MB
Python interpreter:    ~30 MB
TensorFlow runtime:    ~150 MB
Model (FP16, ~500MB): ~500 MB
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
IDLE Total:           ~760 MB > 512 MB ‚ö†Ô∏è V·∫™N H∆†I CAO!
```

**‚Üí V·∫´n cao nh∆∞ng c√≥ th·ªÉ work** (Linux overcommit memory)

---

### 3. Memory Idle (V·ªõi Quantized + Optimized)

V·ªõi c√°c optimizations trong code:
- `compile=False` khi load model
- Disable oneDNN: `TF_ENABLE_ONEDNN_OPTS=0`
- Single thread: `OMP_NUM_THREADS=1`

```
Django base:           ~70 MB
Python interpreter:    ~25 MB
TensorFlow (optimized):~120 MB (thay v√¨ 200MB)
Model (FP16):         ~400 MB (quantized t·ªët)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
IDLE Total:           ~615 MB > 512 MB ‚ö†Ô∏è
```

**‚Üí V·∫´n h∆°i cao, nh∆∞ng c√≥ th·ªÉ work n·∫øu:**
- Linux overcommit memory
- Kh√¥ng c√≥ process n√†o kh√°c
- PythonAnywhere cho ph√©p spike nh·∫π

---

## üî• MEMORY SPIKE KHI INFERENCE (C√≥ Grad-CAM)

### Timeline chi ti·∫øt:

```
t=0s: Idle state (~615 MB)
  ‚Üì
t=0.5s: Nh·∫≠n request, load image
  Memory: ~615 + 10 (image) = ~625 MB
  ‚Üì
t=1s: Preprocess image
  Memory: ~625 + 20 (preprocessing) = ~645 MB
  ‚Üì
t=2s: Model inference
  Memory: ~645 + 50 (inference buffers) = ~695 MB
  ‚Üì
t=3s: Compute Grad-CAM (PEAK!)
  Memory: ~695 + 150 (gradients + heatmap) = ~845 MB ‚ö†Ô∏è PEAK!
  ‚Üì
t=4s: Generate visualization
  Memory: ~845 + 50 (image blending) = ~895 MB
  ‚Üì
t=5s: Cleanup (del heatmap, gc.collect())
  Memory: ~895 ‚Üí ~650 MB (cleanup)
  ‚Üì
t=6s: Return response, cleanup all
  Memory: ~650 ‚Üí ~615 MB (back to idle)
```

**Peak memory: ~895 MB**

**‚Üí 895 MB > 512 MB!** ‚ùå

---

## ü§î V·∫¨Y T·∫†I SAO C√ì TH·ªÇ V·∫™N HO·∫†T ƒê·ªòNG?

### 1. Linux Memory Overcommit

Linux (PythonAnywhere d√πng Linux) c√≥ policy "overcommit":

```python
# /proc/sys/vm/overcommit_memory
# Mode 0 (default): Heuristic overcommit
# Mode 1: Always overcommit (risky)
# Mode 2: Never overcommit
```

**Mode 0 (m·∫∑c ƒë·ªãnh):**
- Cho ph√©p allocate nhi·ªÅu h∆°n RAM physical
- S·ª≠ d·ª•ng swap n·∫øu c·∫ßn
- Ch·ªâ kill process n·∫øu TH·ª∞C S·ª∞ out of memory

**Nghƒ©a l√†:**
- B·∫°n c√≥ th·ªÉ allocate > 512MB
- N·∫øu kh√¥ng th·ª±c s·ª± d√πng h·∫øt ‚Üí OK
- N·∫øu d√πng h·∫øt ‚Üí OOM killer

### 2. Memory l√† Lazy Allocated

Khi b·∫°n malloc(100MB):
- Ch·ªâ reserve virtual memory
- Physical RAM ch·ªâ ƒë∆∞·ª£c allocated khi WRITE
- Nhi·ªÅu buffer kh√¥ng ƒë∆∞·ª£c fully used

### 3. Temporary Spikes ƒë∆∞·ª£c tolerate

PythonAnywhere c√≥ th·ªÉ tolerate temporary spikes:
- Peak trong 1-2 gi√¢y: OK
- Sustained high memory: Kill

---

## üí° V·∫¨Y GRAD-CAM C√ì N√äN B·∫¨T HAY KH√îNG?

### ‚úÖ C√ì TH·ªÇ B·∫¨T N·∫æU:

1. **Model ƒë√£ ƒë∆∞·ª£c quantize < 400MB**
   ```bash
   ls -lh Dermal/*.keras
   # N·∫øu < 400MB ‚Üí GO!
   ```

2. **Code c√≥ memory cleanup** (‚úÖ ƒê√£ c√≥!)
   ```python
   del heatmap, gradients, ...
   gc.collect()
   ```

3. **Timeout ƒë·ªß l·ªõn** (‚úÖ 300s!)
   - Inference + Grad-CAM = ~5-10s
   - V·∫´n c√≤n d∆∞ 290s

4. **Ready to fallback**
   - N·∫øu b·ªã OOM ‚Üí Log s·∫Ω show "Killed"
   - T·∫Øt Grad-CAM v√† restart

---

## üìä RECOMMENDATION M·ªöI

### Cho PythonAnywhere Free (512MB):

#### Scenario 1: Model CH∆ØA quantize (>800MB)

```bash
PRELOAD_MODEL=false      # Lazy load
ENABLE_GRADCAM=false     # ‚ùå T·∫ÆT Grad-CAM
```

**L√Ω do:** Model qu√° l·ªõn, kh√¥ng ƒë·ªß RAM cho c·∫£ model + Grad-CAM

---

#### Scenario 2: Model ƒë√£ quantize (<500MB) ‚Üê USER ƒê√öNG!

```bash
PRELOAD_MODEL=false      # Lazy load
ENABLE_GRADCAM=true      # ‚úÖ B·∫¨T Grad-CAM!
```

**L√Ω do:**
- ‚úÖ Model nh·ªè (~400MB) + TensorFlow (~120MB) = ~520MB idle
- ‚úÖ Peak v·ªõi Grad-CAM: ~750-850MB
- ‚úÖ Temporary spike trong 2-3s ‚Üí Linux c√≥ th·ªÉ tolerate
- ‚úÖ Cleanup sau khi xong ‚Üí v·ªÅ ~520MB
- ‚úÖ Timeout 300s ‚Üí ƒê·ªß th·ªùi gian d∆∞ th·ª´a!

**R·ªßi ro:** C√≥ th·ªÉ b·ªã OOM n·∫øu:
- Linux strict memory limit
- C√≥ nhi·ªÅu concurrent requests (nh∆∞ng PA free ch·ªâ 1 worker)

**C√°ch test:**
```bash
# Monitor memory
curl https://yourusername.pythonanywhere.com/memory-status

# Upload ·∫£nh nhi·ªÅu l·∫ßn
# N·∫øu app b·ªã kill ‚Üí Check error log
# N·∫øu OK ‚Üí Grad-CAM works! ‚úÖ
```

---

#### Scenario 3: Model quantize + t·ªëi ∆∞u (<400MB)

```bash
PRELOAD_MODEL=true       # ‚úÖ C√ì TH·ªÇ pre-load!
ENABLE_GRADCAM=true      # ‚úÖ B·∫¨T Grad-CAM!
```

**L√Ω do:**
- Model < 400MB ‚Üí Idle ~500MB
- C√≤n d∆∞ ~12MB buffer
- Peak v·ªõi Grad-CAM: ~700MB (c√≥ th·ªÉ tolerate)
- Pre-load ‚Üí Request ƒë·∫ßu ti√™n c≈©ng nhanh!

**ƒêi·ªÅu ki·ªán:**
- ‚úÖ Model ƒë√£ quantize FP16
- ‚úÖ T·∫Øt c√°c t√≠nh nƒÉng kh√¥ng c·∫ßn thi·∫øt
- ‚úÖ Monitor memory carefully

---

## üéØ V·∫¨Y N√äN L√ÄM G√å?

### B∆∞·ªõc 1: Quantize Model (N·∫æU CH∆ØA)

```bash
# Check size hi·ªán t·∫°i
ls -lh Dermal/dermatology_stage1.keras

# N·∫øu > 500MB ‚Üí Quantize
python Dermal/quantize_model.py
```

### B∆∞·ªõc 2: Test v·ªõi Grad-CAM B·∫¨T

```bash
# .env
PRELOAD_MODEL=false
ENABLE_GRADCAM=true  # ‚Üê TH·ª¨ B·∫¨T!
```

### B∆∞·ªõc 3: Deploy v√† Monitor

```bash
# Upload l√™n PA
# Upload v√†i ·∫£nh test
# Check memory status

curl https://yourusername.pythonanywhere.com/memory-status

# Response:
{
  "memory": {
    "rss_mb": 487.3  # < 512 = OK! ‚úÖ
  }
}
```

### B∆∞·ªõc 4: Check Error Log

N·∫øu app b·ªã kill:
```
Dashboard ‚Üí Web ‚Üí Error log
# Th·∫•y "Killed" ‚Üí RAM qu√° cao ‚Üí T·∫Øt Grad-CAM
```

N·∫øu kh√¥ng b·ªã kill:
```
‚úÖ GRAD-CAM WORKS! Gi·ªØ nguy√™n config!
```

---

## üìù T√ìM T·∫ÆT

### ‚ùå TR∆Ø·ªöC (Suy nghƒ© c·ªßa t√¥i - QU√Å BI QUAN):

```
PA 512MB ‚Üí T·∫Øt m·ªçi th·ª© ƒë·ªÉ an to√†n
‚Üí ENABLE_GRADCAM=false
```

### ‚úÖ SAU (User ƒë√∫ng!):

```
PA c√≥ 300s timeout + Linux overcommit
  ‚Üì
N·∫øu model < 400MB (quantized)
  ‚Üì
ENABLE_GRADCAM=true ‚úÖ WORKS!
  ‚Üì
Peak memory ~750-850MB
  ‚Üì
Temporary spike (2-3s) ‚Üí Linux tolerate
  ‚Üì
Cleanup ‚Üí Back to ~520MB
  ‚Üì
App kh√¥ng b·ªã kill ‚úÖ
```

**User ƒê√öNG R·ªíI!** 

V·ªõi model quantized + 300s timeout ‚Üí **C√ì TH·ªÇ V√Ä N√äN b·∫≠t Grad-CAM!** üéâ

---

## üöÄ UPDATED RECOMMENDATION

### PythonAnywhere Free (512MB) - OPTIMAL CONFIG:

```bash
# .env
PRELOAD_MODEL=false          # Lazy load (ti·∫øt ki·ªám RAM l√∫c start)
ENABLE_GRADCAM=true          # ‚úÖ B·∫¨T! (n·∫øu model < 500MB)

# TensorFlow optimizations
TF_CPP_MIN_LOG_LEVEL=3
TF_ENABLE_ONEDNN_OPTS=0
OMP_NUM_THREADS=1
```

**ƒêi·ªÅu ki·ªán:**
- Model ƒë√£ quantize < 500MB
- Monitor memory sau khi deploy
- Ready to t·∫Øt Grad-CAM n·∫øu b·ªã OOM

**K·∫øt qu·∫£:**
- ‚úÖ Request ƒë·∫ßu: ~65s (load model + Grad-CAM)
- ‚úÖ Request ti·∫øp: ~8-12s (v·ªõi Grad-CAM)
- ‚úÖ User experience t·ªët (c√≥ visualization)
- ‚úÖ Kh√¥ng b·ªã timeout (300s >> 12s)
- ‚úÖ C√≥ th·ªÉ fit trong 512MB (v·ªõi quantized model)
