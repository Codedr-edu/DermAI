# ğŸ”’ PHÃ‚N TÃCH AN TOÃ€N: PRE-LOAD MODEL

## ğŸ“‹ TÃ“M Táº®T

Document nÃ y giáº£i thÃ­ch chi tiáº¿t:
1. âœ… CÃ¡c váº¥n Ä‘á» tiá»m áº©n khi pre-load model
2. âœ… CÃ¡ch code Ä‘Ã£ Ä‘Æ°á»£c sá»­a Ä‘á»ƒ an toÃ n
3. âœ… CÃ¡ch test vÃ  verify
4. âœ… CÃ¡c edge cases Ä‘Ã£ xá»­ lÃ½

---

## ğŸš¨ CÃC Váº¤N Äá»€ ÄÃƒ PHÃT HIá»†N VÃ€ KHáº®C PHá»¤C

### 1. âŒ Váº¤N Äá»€: Migration Conflict

**MÃ´ táº£:**
```bash
python manage.py migrate
â†“
Django import apps.py
â†“
ready() Ä‘Æ°á»£c gá»i â†’ Load model (60s)
â†“
NhÆ°ng chÆ°a cáº§n model khi migrate!
```

**Há»‡ quáº£:**
- Build cháº­m (thÃªm 60s khÃ´ng cáº§n thiáº¿t)
- CÃ³ thá»ƒ crash náº¿u model cáº§n DB

**âœ… GIáº¢I PHÃP:**

```python
# Trong apps.py
skip_commands = {
    'migrate', 'makemigrations', 'createsuperuser', 
    'shell', 'test', 'collectstatic', ...
}

if len(sys.argv) > 1:
    command = sys.argv[1]
    if command in skip_commands:
        print(f"â„¹ï¸ Skipping model pre-load (running: {command})")
        return  # â† KHÃ”NG LOAD MODEL
```

**Test:**
```bash
# NÃªn tháº¥y message "Skipping model pre-load"
python manage.py migrate
python manage.py collectstatic
```

---

### 2. âŒ Váº¤N Äá»€: Gunicorn Fork + TensorFlow Conflict

**MÃ´ táº£:**

TensorFlow táº¡o threads vÃ  GPU contexts. Khi Gunicorn fork workers:

```
WITHOUT --preload-app (Máº¶C Äá»ŠNH):
Master: Load Django (khÃ´ng load model)
  â†“ fork() Worker 1
  â†“   â†’ ready() Ä‘Æ°á»£c gá»i â†’ Load TF model (500MB RAM)
  â†“ fork() Worker 2  
  â†“   â†’ ready() Ä‘Æ°á»£c gá»i â†’ Load TF model (500MB RAM)
  
â†’ RAM usage: 500MB Ã— workers (Tá»N RAM!)
â†’ CÃ³ thá»ƒ bá»‹ TensorFlow thread deadlock

WITH --preload-app:
Master: Load Django â†’ ready() â†’ Load TF model (500MB RAM)
  â†“ fork() Worker 1 â†’ Chia sáº» memory
  â†“ fork() Worker 2 â†’ Chia sáº» memory
  
â†’ RAM usage: 500MB total (TIáº¾T KIá»†M!)
â†’ NhÆ°ng cáº©n tháº­n: TensorFlow cÃ³ thá»ƒ khÃ´ng thÃ­ch fork()
```

**Váº¥n Ä‘á» vá»›i TensorFlow + fork():**
- TensorFlow táº¡o threads trÆ°á»›c khi fork
- Sau fork, threads bá»‹ duplicate â†’ deadlock
- CUDA context khÃ´ng há»£p lá»‡ sau fork

**âœ… GIáº¢I PHÃP:**

```yaml
# render.yaml
startCommand: "gunicorn ... --workers 1 --preload-app"
                                    â†‘           â†‘
                        Chá»‰ 1 worker   Load trÆ°á»›c khi fork
```

**Táº¡i sao an toÃ n:**
- `--workers 1`: KhÃ´ng cÃ³ fork, khÃ´ng cÃ³ conflict âœ…
- `--preload-app`: Model load 1 láº§n, tiáº¿t kiá»‡m RAM âœ…
- Náº¿u sau nÃ y cáº§n scale lÃªn 2+ workers:
  - Option A: Bá» `--preload-app`, Ä‘á»ƒ má»—i worker load riÃªng (tá»‘n RAM hÆ¡n nhÆ°ng an toÃ n)
  - Option B: DÃ¹ng `tensorflow.compat.v1.disable_eager_execution()` Ä‘á»ƒ trÃ¡nh thread issues

---

### 3. âŒ Váº¤N Äá»€: ready() Ä‘Æ°á»£c gá»i nhiá»u láº§n

**MÃ´ táº£:**

Django cÃ³ thá»ƒ gá»i `ready()` nhiá»u láº§n:
- Láº§n 1: Khi Django setup
- Láº§n 2: Khi autoreload (runserver)
- Láº§n 3: Trong tests

**Há»‡ quáº£:**
- Load model 2-3 láº§n â†’ tá»‘n thá»i gian, RAM

**âœ… GIáº¢I PHÃP:**

```python
class DermalConfig(AppConfig):
    _model_preloaded = False  # â† Class variable (shared)
    
    def ready(self):
        # Check trÆ°á»›c khi load
        if DermalConfig._model_preloaded:
            return  # â† ÄÃ£ load rá»“i, skip!
        
        # ... load model ...
        
        DermalConfig._model_preloaded = True  # â† ÄÃ¡nh dáº¥u Ä‘Ã£ load
```

**Káº¿t há»£p vá»›i thread lock trong AI_detection.py:**

```python
def get_model():
    if _loaded_model is not None:
        return _loaded_model  # â† Double protection
    
    with _model_lock:  # â† Thread-safe
        if _loaded_model is not None:
            return _loaded_model
        
        _loaded_model = load_model(...)
```

â†’ **2 táº§ng báº£o vá»‡:** Class variable + Thread lock âœ…

---

### 4. âŒ Váº¤N Äá»€: Memory Overflow (OOM)

**MÃ´ táº£:**

Render free tier: **512MB RAM**

```
Django base:        ~100MB
TensorFlow runtime: ~200MB
Model loaded:       ~1000MB  â† Model cá»§a báº¡n
Inference buffers:  ~200MB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:              ~1500MB >>> 512MB âŒ OOM KILL!
```

**âœ… GIáº¢I PHÃP:**

```python
# apps.py - In ra memory usage Ä‘á»ƒ monitor
try:
    import psutil
    mem_mb = process.memory_info().rss / (1024 * 1024)
    print(f"ğŸ“Š Memory usage after model load: {mem_mb:.1f} MB")
except ImportError:
    pass
```

**Environment variables Ä‘á»ƒ control:**

```yaml
# render.yaml
- key: PRELOAD_MODEL
  value: true  # â† Set false náº¿u bá»‹ OOM

- key: ENABLE_GRADCAM
  value: true  # â† Set false Ä‘á»ƒ tiáº¿t kiá»‡m RAM (~200MB)
```

**Náº¿u váº«n bá»‹ OOM:**

1. **Táº¯t pre-loading:**
   ```
   PRELOAD_MODEL=false
   ```
   â†’ Request Ä‘áº§u tiÃªn sáº½ cháº­m, nhÆ°ng khÃ´ng bá»‹ OOM

2. **Táº¯t Grad-CAM:**
   ```
   ENABLE_GRADCAM=false
   ```
   â†’ Tiáº¿t kiá»‡m ~200MB RAM

3. **Quantize model:**
   ```python
   # Chuyá»ƒn tá»« FP32 â†’ FP16
   model = tf.keras.models.load_model(...)
   model = tf.quantization.quantize(model, ...)
   ```
   â†’ Giáº£m size 50%

4. **Upgrade Render plan:**
   - Free: 512MB RAM
   - Starter: 2GB RAM (~$7/month)

---

### 5. âŒ Váº¤N Äá»€: sys.argv detection khÃ´ng chÃ­nh xÃ¡c

**MÃ´ táº£:**

Code cÅ©:
```python
if 'gunicorn' in sys.argv[0]:
    # Load model
```

**CÃ¡c case bá»‹ miss:**
```bash
python -m gunicorn ...  # sys.argv[0] = "python", khÃ´ng pháº£i "gunicorn"
./start.sh              # sys.argv[0] = "start.sh"
```

**âœ… GIáº¢I PHÃP:**

```python
# Nhiá»u phÆ°Æ¡ng phÃ¡p detect, khÃ´ng chá»‰ dá»±a vÃ o sys.argv[0]

# Method 1: argv[0]
if 'gunicorn' in sys.argv[0].lower():
    is_server = True

# Method 2: Check argv
if 'runserver' in sys.argv:
    is_server = True

# Method 3: Environment variable (RELIABLE NHáº¤T!)
if os.getenv('DJANGO_SERVER_MODE') == 'true':
    is_server = True

# Method 4: WSGI environment
if os.getenv('WSGI_APPLICATION'):
    is_server = True
```

â†’ **DÃ¹ng 4 phÆ°Æ¡ng phÃ¡p káº¿t há»£p** Ä‘á»ƒ Ä‘áº£m báº£o khÃ´ng miss case nÃ o âœ…

---

### 6. âœ… Circular Import (ÄÃƒ KIá»‚M TRA, OK)

**Kiá»ƒm tra:**
```python
# AI_detection.py cÃ³ import models.py khÃ´ng?
grep -r "from .models import" Dermal/AI_detection.py
# â†’ KhÃ´ng cÃ³ âœ…

# AI_detection.py import gÃ¬?
# â†’ Chá»‰ import tensorflow, PIL, numpy
# â†’ KhÃ´ng cÃ³ circular import âœ…
```

---

## ğŸ“Š SO SÃNH CODE CÅ¨ VÃ€ Má»šI

### âŒ CODE CÅ¨ (KhÃ´ng an toÃ n):

```python
# apps.py - Code cÅ©
def ready(self):
    import sys
    if 'runserver' in sys.argv or 'gunicorn' in sys.argv[0]:
        from . import AI_detection
        model = AI_detection.get_model()
```

**Váº¥n Ä‘á»:**
- âŒ KhÃ´ng skip migrations
- âŒ KhÃ´ng prevent duplicate loads
- âŒ sys.argv check khÃ´ng Ä‘á»§
- âŒ KhÃ´ng handle errors
- âŒ KhÃ´ng monitor memory

### âœ… CODE Má»šI (An toÃ n):

```python
# apps.py - Code má»›i
class DermalConfig(AppConfig):
    _model_preloaded = False  # â† Prevent duplicate
    
    def ready(self):
        # 1. Check duplicate
        if DermalConfig._model_preloaded:
            return
        
        # 2. Check environment variable
        if not os.getenv('PRELOAD_MODEL', 'true') == 'true':
            return
        
        # 3. Skip management commands
        skip_commands = {'migrate', 'test', ...}
        if sys.argv[1] in skip_commands:
            return
        
        # 4. Multiple detection methods
        is_server = (
            'gunicorn' in sys.argv[0] or
            'runserver' in sys.argv or
            os.getenv('DJANGO_SERVER_MODE') == 'true' or
            os.getenv('WSGI_APPLICATION')
        )
        
        if not is_server:
            return
        
        # 5. Load with error handling
        try:
            model = AI_detection.get_model()
            DermalConfig._model_preloaded = True
            
            # 6. Monitor memory
            import psutil
            print(f"ğŸ“Š Memory: {mem_mb:.1f} MB")
        except Exception as e:
            print(f"âš ï¸ Failed: {e}")
```

**Cáº£i thiá»‡n:**
- âœ… Skip migrations/tests
- âœ… Prevent duplicate loads
- âœ… 4 phÆ°Æ¡ng phÃ¡p detect server
- âœ… Full error handling
- âœ… Memory monitoring
- âœ… Fail gracefully (fallback to lazy loading)

---

## ğŸ§ª CÃCH TEST

### Test 1: Migration khÃ´ng load model

```bash
python manage.py migrate

# Mong Ä‘á»£i:
# â„¹ï¸ Skipping model pre-load (running: migrate)
# (KHÃ”NG tháº¥y "ğŸ”„ Loading model...")
```

### Test 2: Runserver cÃ³ load model

```bash
python manage.py runserver

# Mong Ä‘á»£i:
# ğŸš€ Pre-loading AI model...
# ğŸ”„ Loading model from: ...
# âœ… Model pre-loaded successfully: Functional
# ğŸ“Š Memory usage after model load: 1234.5 MB
```

### Test 3: Health check

```bash
curl http://localhost:8000/health?verbose=1

# Response:
{
  "status": "ok",
  "model_loaded": true,
  "message": "Model pre-loaded, ready for inference"
}
```

### Test 4: Request nhanh

```bash
# Request Ä‘áº§u tiÃªn (sau cold start)
time curl -F "image=@test.jpg" http://localhost:8000/upload

# Mong Ä‘á»£i: < 10 giÃ¢y (khÃ´ng bá»‹ timeout)
```

### Test 5: Memory khÃ´ng overflow

```bash
# Trong Render dashboard, xem memory usage
# Náº¿u > 80% cá»§a 512MB â†’ Nguy hiá»ƒm, cáº§n optimize
```

---

## ğŸ¯ Káº¾T LUáº¬N

### âœ… ÄÃƒ Xá»¬ LÃ:

1. âœ… Migration conflict (skip commands)
2. âœ… Gunicorn fork issues (--preload-app + 1 worker)
3. âœ… Duplicate loads (class variable)
4. âœ… Memory monitoring (psutil)
5. âœ… Robust detection (4 methods)
6. âœ… Error handling (graceful fallback)
7. âœ… Environment variable control (PRELOAD_MODEL)

### ğŸ”’ AN TOÃ€N:

- âœ… KhÃ´ng load trong migrations
- âœ… KhÃ´ng load nhiá»u láº§n
- âœ… KhÃ´ng crash náº¿u load fail
- âœ… CÃ³ thá»ƒ táº¯t pre-loading náº¿u cáº§n
- âœ… Monitor Ä‘Æ°á»£c memory usage

### ğŸ“ˆ Káº¾T QUáº¢:

**TrÆ°á»›c:**
- Request Ä‘áº§u tiÃªn: 80-115s â†’ **TIMEOUT** âŒ

**Sau:**
- Request Ä‘áº§u tiÃªn: 3-10s â†’ **THÃ€NH CÃ”NG** âœ…
- CÃ¡c request tiáº¿p theo: 2-5s âœ…

### âš ï¸ LÆ¯U Ã:

1. **RAM háº¡n cháº¿ (512MB):**
   - Náº¿u model quÃ¡ lá»›n â†’ OOM
   - Giáº£i phÃ¡p: Táº¯t GRADCAM, quantize model, hoáº·c upgrade plan

2. **Náº¿u sau nÃ y scale lÃªn 2+ workers:**
   - Bá» `--preload-app` Ä‘á»ƒ trÃ¡nh TensorFlow fork issues
   - Hoáº·c disable eager execution

3. **Monitor Render logs:**
   ```bash
   render logs
   # Xem cÃ³ message "âœ… Model pre-loaded" khÃ´ng
   # Xem memory usage cÃ³ quÃ¡ cao khÃ´ng
   ```

---

## ğŸš€ DEPLOY

Sau khi sá»­a code:

```bash
git add Dermal/apps.py render.yaml
git commit -m "Fix: Add safe model pre-loading with migration/fork protection"
git push
```

Render sáº½ tá»± Ä‘á»™ng deploy. Check logs:

```
[build] Running: ./build.sh
[build] python manage.py migrate
[build] â„¹ï¸ Skipping model pre-load (running: migrate)  â† GOOD
[start] ğŸš€ Pre-loading AI model...
[start] âœ… Model pre-loaded successfully: Functional    â† GOOD
[start] ğŸ“Š Memory usage after model load: 987.3 MB
```

Done! ğŸ‰
