# Pre-loading Model Ä‘á»ƒ trÃ¡nh Timeout trÃªn Render Free Tier

## Váº¥n Ä‘á»

Khi deploy lÃªn Render free tier, instance sáº½ "spin down" (táº¯t) sau 15 phÃºt khÃ´ng hoáº¡t Ä‘á»™ng. Khi cÃ³ request má»›i:

1. **Cold start**: Server khá»Ÿi Ä‘á»™ng láº¡i (~50 giÃ¢y)
2. **Model loading**: Load model TensorFlow vÃ o RAM (~30-60 giÃ¢y vá»›i model lá»›n)
3. **Inference**: Xá»­ lÃ½ dá»± Ä‘oÃ¡n (~2-5 giÃ¢y)

**Tá»•ng thá»i gian: 80-115 giÃ¢y** âŒ

NhÆ°ng HTTP request timeout thÆ°á»ng chá»‰ **30-60 giÃ¢y** â†’ Request bá»‹ **502/504 timeout**!

## Giáº£i phÃ¡p

### 1. Pre-load Model khi App Khá»Ÿi Ä‘á»™ng

File `Dermal/apps.py` Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘á»ƒ tá»± Ä‘á»™ng load model khi Django khá»Ÿi Ä‘á»™ng:

```python
class DermalConfig(AppConfig):
    def ready(self):
        # Load model ngay khi server start
        model = AI_detection.get_model()
```

**Æ¯u Ä‘iá»ƒm:**
- Request Ä‘áº§u tiÃªn chá»‰ máº¥t ~2-5s (chá»‰ inference, khÃ´ng load model)
- KhÃ´ng bá»‹ timeout
- User experience tá»‘t hÆ¡n

**Timeline má»›i:**
1. Cold start: ~50s
2. **Model Ä‘Ã£ Ä‘Æ°á»£c load sáºµn** âœ…
3. Request Ä‘áº§u tiÃªn: ~2-5s â†’ **ThÃ nh cÃ´ng!** âœ…

### 2. Environment Variable

CÃ³ thá»ƒ báº­t/táº¯t pre-loading báº±ng biáº¿n mÃ´i trÆ°á»ng:

```bash
# Báº­t pre-loading (máº·c Ä‘á»‹nh)
PRELOAD_MODEL=true

# Táº¯t pre-loading (náº¿u cáº§n tiáº¿t kiá»‡m RAM)
PRELOAD_MODEL=false
```

### 3. Health Check

Endpoint `/health` giá» sáº½ bÃ¡o model Ä‘Ã£ load chÆ°a:

```bash
# Simple check
curl https://your-app.onrender.com/health
# Response: ok (model: loaded)

# Verbose check
curl https://your-app.onrender.com/health?verbose=1
# Response: {"status": "ok", "model_loaded": true, "message": "Model pre-loaded, ready for inference"}
```

### 4. Memory Status

Endpoint `/memory-status` Ä‘á»ƒ monitor RAM usage:

```bash
curl https://your-app.onrender.com/memory-status
```

## LÆ°u Ã½

### RAM Usage
- Model chiáº¿m ~500MB-2GB RAM (tÃ¹y model size)
- Render free tier cÃ³ **512MB RAM** â†’ CÃ³ thá»ƒ khÃ´ng Ä‘á»§ náº¿u model quÃ¡ lá»›n
- Giáº£i phÃ¡p:
  - Quantize model (FP16 thay vÃ¬ FP32)
  - Táº¯t Grad-CAM náº¿u khÃ´ng cáº§n: `ENABLE_GRADCAM=false`
  - Upgrade lÃªn Render paid tier (cÃ³ 2GB+ RAM)

### Cold Start váº«n cháº­m
- Pre-loading chá»‰ giáº£i quyáº¿t váº¥n Ä‘á» **timeout**
- Cold start váº«n máº¥t ~50s (do Render spin up container)
- User váº«n pháº£i Ä‘á»£i ~50-55s cho request Ä‘áº§u tiÃªn (nhÆ°ng khÃ´ng bá»‹ timeout)
- CÃ¡c request tiáº¿p theo sáº½ nhanh (<5s) cho Ä‘áº¿n khi instance sleep láº¡i

### Development
- Trong development, cÃ³ thá»ƒ táº¯t pre-loading Ä‘á»ƒ khá»Ÿi Ä‘á»™ng nhanh hÆ¡n:
  ```bash
  PRELOAD_MODEL=false python manage.py runserver
  ```

## Kiá»ƒm tra

### Test Pre-loading hoáº¡t Ä‘á»™ng

1. **Start server vÃ  xem log:**
   ```bash
   python manage.py runserver
   ```
   
   Báº¡n sáº½ tháº¥y:
   ```
   ğŸš€ Pre-loading AI model to avoid timeout on first request...
   ğŸ”„ Loading model from: /path/to/model.keras
   âœ… Loaded model: Functional name: model
   âœ… Model warmed up
   âœ… Model pre-loaded successfully: Functional
   ```

2. **Kiá»ƒm tra health:**
   ```bash
   curl http://localhost:8000/health?verbose=1
   ```
   
   Káº¿t quáº£ mong Ä‘á»£i:
   ```json
   {
     "status": "ok",
     "model_loaded": true,
     "message": "Model pre-loaded, ready for inference"
   }
   ```

3. **Test request Ä‘áº§u tiÃªn nhanh:**
   - Upload áº£nh láº§n Ä‘áº§u tiÃªn
   - Response time nÃªn < 10 giÃ¢y (thay vÃ¬ 30-60s)

## Káº¿t luáº­n

âœ… **TrÆ°á»›c Ä‘Ã¢y**: Request Ä‘áº§u tiÃªn bá»‹ timeout vÃ¬ load model trong request (>80s)

âœ… **BÃ¢y giá»**: Model Ä‘Æ°á»£c load sáºµn khi server start, request Ä‘áº§u tiÃªn chá»‰ máº¥t vÃ i giÃ¢y

âš ï¸ **Trade-off**: Tá»‘n nhiá»u RAM hÆ¡n (model luÃ´n á»Ÿ trong memory), cÃ³ thá»ƒ cáº§n upgrade plan náº¿u model quÃ¡ lá»›n
